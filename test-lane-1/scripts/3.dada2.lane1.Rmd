---
title: "dada2"
author: "M Fisher, via Eily via Moncho"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: yes
---

# Description




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if(!require("here")) {install.packages("here")}
if(!require("tidyverse")) {install.packages("tidyverse")}
if(!require("magrittr")) {install.packages("magrittr")}
if(!require("digest")) {install.packages("digest")}
if(!require("seqinr")) {install.packages("seqinr")}

# if(!requireNamespace("BiocManager", quietly = TRUE)){install.packages("BiocManager")}
# BiocManager::install("dada2", version = "3.10")

library(dada2)
library(digest)
library(seqinr)
```
<br>

User directories and inputs
```{r set up }
# root directory for cutadapt
cutadapt_dir <- "test-lane-1/data/cutadapt"

# output directory
outdir <- "test-lane-1/data/dada2"

# vector of different markers, as they appear in directories / metadata file
markers <- c("BF3","Leray","LerayXT")
```


# BF3

## Data

read in sequencing metadata, file names.
```{r message=FALSE}
marker <- markers[1]
marker.meta <- read_csv(here(cutadapt_dir, marker, paste0("cutadapt_output_metadata_",marker,".csv")))

## forward and reverse filenames
fnFs <- sort(list.files(path=here(cutadapt_dir,marker), pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path=here(cutadapt_dir,marker), pattern="_R2_001.fastq", full.names = TRUE))
```
<br>

get the sample names, which will be used to name the filtered files.
```{r}
# sample.names <- marker.meta$Sample_name
prefix <- paste0("Locus_",marker[1],"_")
suffix <- paste0("_L001_R1_001.fastq.fastq")
sample.names <- str_remove(marker.meta$file1, prefix)
sample.names <- str_remove(sample.names,suffix)
```
<br>

Plot forward read quality scores, one sample per group
```{r echo=FALSE}
plotQualityProfile(fnFs[c(1,2,6,19,25)])
```
<br>

Plot forward read quality scores, one sample per group
```{r echo=FALSE}
plotQualityProfile(fnRs[c(1,2,6,19,25)])
```
<br>

So the kangaroo, WACO.2021, and WASS.2020 samples look pretty good until ~130bp. The WASS.2018 sample is pretty atrocious, and my baby dungy sample isn't looking much better. Another red flag is how few reads I have left after running `cutadapt`...

## Filter and trim

name the filtered sequencing data
```{r}
filtFs <- file.path(here(outdir, paste0(sample.names, "_F_filt.fastq.gz")))
filtRs <- file.path(here(outdir, paste0(sample.names, "_R_filt.fastq.gz")))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```
<br>

Filter and trim using `dada2`.

- `truncLen` truncates the sequence length, and should be based on per-base quality scores
- `maxEE` discards reads with high "expected errors" = $\sum(10^{-Q/10})$
- `rm.phix` removes reads from PhiX spike
- use `compress` to gzip the output fastqs
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(120,120),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```
<br> 

Before `dada2` can learn error rates, the files that have 0 reads need to be removed from the file list. 
```{r}
files_to_remove <- as.data.frame(out) %>% 
  rownames_to_column() %>%
  filter(reads.out==0) %>%
  mutate(sample_names=str_remove(rowname, paste0("Locus_",marker,"_"))) %>%
  mutate(sample_names=str_remove(sample_names,paste0("_L001_R1_001.fastq.fastq"))) %>%
  separate(sample_names, into=c("id","sample_number"), sep=-2, remove=FALSE)

filtFs <- filtFs[-which(names(filtFs) %in% files_to_remove$sample_names)]
filtRs <- filtRs[-which(names(filtRs) %in% files_to_remove$sample_names)]
marker.meta %<>% filter(!(Sample_name %in% files_to_remove$sample_number))
```

```{r eval=FALSE}
errF <- learnErrors(filtFs)
errR <- learnErrors(filtRs)
plotErrors(errF, nominalQ=TRUE)

dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

# check it 
dadaFs[[1]]

# we are going to SKIP merging paired end reads for now because there is no overlap here 
#mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

# there is another term to do this so we have it formatted right to not merge them
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE, justConcatenate = TRUE)
## Inspect the merger data.frame from the first sample
head(mergers[[1]])
```
***Error in derepFastq(fls[[i]], qualityType = qualityType) : Not all provided files exist.***



```{r make sequence table, eval=FALSE}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
# so here all of our sequences are 250 bp in length 

# remove chimeras 
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
# check percentage of reads remaining after chimera removal (should be high)
sum(seqtab.nochim)/sum(seqtab)
seqtab.nochim.df <- as.data.frame(seqtab.nochim)


# track through pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
#track <- cbind(out, getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)

```

```{r setting up output files, eval=FALSE}

# Set up output files 
output.dir <- "/Users/elizabethandruszkiewicz/GoogleDrive/UW/GitHub/NextGenNEPA_EA/Output/dada2_output/MFtest/LerayXT"
conv_file <- file.path(output.dir,"F_only_hash_key.csv")
conv_file.fasta <- file.path(output.dir,"F_only_hash_key.fasta")
ASV_file <-  file.path(output.dir,"F_only_ASV_table.csv")
print (conv_file)
print (conv_file.fasta)
print(ASV_file)

# actually write output
conv_table <- tibble( Hash = "", Sequence ="")
Hashes <- map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto")) 
conv_table <- tibble (Hash = Hashes,
                      Sequence = colnames(seqtab.nochim.df))

colnames(seqtab.nochim.df) <- Hashes
write_csv(conv_table, conv_file) # write the table into a file
write.fasta(sequences = as.list(conv_table$Sequence),
            names     = as.list(conv_table$Hash),
            file.out = conv_file.fasta)
seqtab.nochim.df <- bind_cols(sample.metadata %>%
                                select(Sample_name, Locus),
                              seqtab.nochim.df)
current_asv <- 
  seqtab.nochim.df %>%
  pivot_longer(cols = c(- Sample_name, - Locus),
               names_to = "Hash",
               values_to = "nReads") %>%
  filter(nReads > 0) 
write_csv(current_asv, ASV_file)
```