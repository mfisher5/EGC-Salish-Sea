---
title: "DADA2 pooled errors"
subtitle: "last run: `r format(Sys.time(), '%B %d, %Y')`"
author: "M Fisher (from Eily, Moncho)"
date: '2022-10-12'
output: 
  html_document:
    toc: yes
---


# Description

Run **DADA2** [tutorial here](https://benjjneb.github.io/dada2/tutorial.html) in order to get an amplicon sequence variant (ASV) table, which records the number of times each exact amplicon sequence variant was observed in each sample. 

Certain decisions have to be made throughout the script, so *do not just knit this script with the existing values*. Go through each code chunk in R first, then knit. Certain code chunks will not re-run when the script is knitted, to avoid over-writing existing files.

Input: demultiplexed fastq files, without barcodes / adapters / primers. 

This version of the script used **pooled processing, in which all samples are pooled together for sample inference**. According to the DADA2 tutorial: 

> Independent sample processing has two major advantages: Computation time is linear in the number of samples, and memory requirements are flat with the number of samples. However, pooling allows information to be shared across samples, which makes it easier to resolve rare variants that were present as singletons or doubletone in one sample but were present many times across samples. Pooled sample inference is supported by calling dada(..., pool=TRUE)...  In practice, pooled processing can be used for Miseq scale data (especially if taking advantage of multithreading) 



<br>

# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("here")) {install.packages("here")}
if(!require("tidyverse")) {install.packages("tidyverse")}
if(!require("magrittr")) {install.packages("magrittr")}
if(!require("digest")) {install.packages("digest")}
if(!require("seqinr")) {install.packages("seqinr")}
# if(!requireNamespace("BiocManager", quietly = TRUE)){install.packages("BiocManager")}
# BiocManager::install("dada2", version = "3.10")
library(dada2)
library(digest)
library(seqinr)
```
<br>

User directories
```{r set up }
# root directory for cutadapt
cutadapt_dir <- "data/cutadapt"
# output directory
outdir <- "data/dada2/pooled_process"
```

User inputs
```{r}
run.num = 2

hash = TRUE  # rarely do you want hash = false (EA)

keep.mid.files = FALSE # I find I never look at these / use these and they just take up space (EA)
```
<br>
<br>


# Prep for DADA2

```{r message=FALSE, warning=FALSE}
run_cutadapt_dir = paste0(cutadapt_dir, "/noprimers")
```
<br>

read in sequencing metadata. set default trim length based on primer. 
```{r message=FALSE}
cutadapt.meta <- read_csv(here(run_cutadapt_dir, paste0("output.metadata.csv")))
marker        <- unique(cutadapt.meta$Locus)
print(marker)
```
```{r echo=FALSE}
if(marker=="Leray" | marker=="LerayXT"){
  trimming.length.r1 = 250
  trimming.length.r2 = 200
  message("trim lengths set as (r1,r1): ", trimming.length.r1, ",",trimming.length.r2)
} else if(marker=="BF3"){
  trimming.length.r1 = 260
  trimming.length.r2 = 200
} else{
  message("please manually enter trim length for this marker.")
}
```
<br>

read in file names.
```{r}
fnFs <- sort(list.files(path=here(run_cutadapt_dir), pattern="_R1_001.fastq.fastq", full.names = TRUE))
fnFs_simple <- str_remove(fnFs,pattern=paste0(here(run_cutadapt_dir),"/"))
fnRs <- sort(list.files(path=here(run_cutadapt_dir), pattern="_R2_001.fastq.fastq", full.names = TRUE))
```
<br>

get the sample names, which will be used to name the filtered files.
```{r}
# sample.names <- marker.meta$Sample_name
prefix <- paste0("Locus_Leray_")
suffix <- paste0("_L001_R1_001.fastq.fastq")
sample.names.df <- (cutadapt.meta %>% dplyr::select(file1) %>%
                      filter(file1 %in% fnFs_simple) %>%
                    mutate(sample_id=str_remove(file1,prefix)) %>%
                      mutate(sample_id=str_remove(sample_id,suffix)) %>%
                      separate(col=sample_id, into=c("sample_id","sample.num"), sep="_S")) %>% dplyr::select(sample_id)
sample.names <- as.character(sample.names.df$sample_id)
```
<br>

remove any files from the cutadapt metadata that don't have corresponding files in the post-cutadapt folder (the indexes that weren't used in the run)
```{r}
cutadapt.meta.output <- filter(cutadapt.meta, file1 %in% fnFs_simple)
```
<br>

write output directory path for filtered files in the run's cutadapt folder.
```{r}
filt.dir <- paste0(cutadapt_dir, "/noprimers_filtered_pooled")
```
<br>

create directories if they don't exist
```{r}
if(!dir.exists(here(filt.dir))){
  dir.create(path = here(filt.dir),recursive = T)
}
if(!dir.exists(here(outdir))){
  dir.create(path = here(outdir),recursive = T)
}
```
<br>

Double check the quality scores
Plot forward read quality scores, one sample per group
```{r echo=FALSE}
plotQualityProfile(fnFs[c(1,2,6,19,25)])
```
<br>

Plot forward read quality scores, one sample per group
```{r echo=FALSE}
plotQualityProfile(fnRs[c(1,2,6,19,25)])
```
<br>


manually enter trim lengths - if there isn't a default for the marker, or if the defaults are too long based on the quality scores (see script 0_qc)
```{r eval=FALSE}
trimming.length.r1 = 250
trimming.length.r2 = 190
```
<br>
<br>

# DADA2

## Filter and trim

- `truncLen` truncates the sequence length, and should be based on per-base quality scores. I'm using the length that Eily suggested , 120bp.
- `maxEE` discards reads with high "expected errors" = $\sum(10^{-Q/10})$
- `rm.phix` removes reads from PhiX spike
- use `compress` to gzip the output fastqs

```{r}
output.dada2 <- cutadapt.meta.output %>%
  #filter(rc == 1) %>% # ONLY SELECT THE BACKWARDS ONES (1) OR FORWARDS ONES (0)
  mutate(basename=sample.names) %>%
  mutate(file1  = here(run_cutadapt_dir, file1),
         file2  = here(run_cutadapt_dir, file2),
         filtF1 = here(filt.dir, paste0(basename, "_F1_filt.fastq.gz")),
         filtR1 = here(filt.dir, paste0(basename, "_R1_filt.fastq.gz"))) %>%
  select(-basename) %>% 
  mutate (outFs = pmap(.l= list (file1, filtF1, file2, filtR1),
                       .f = function(a, b, c, d) {
                         filterAndTrim(a,b,c,d,
                                       truncLen=c(trimming.length.r1,trimming.length.r2),
                                       maxN=0, maxEE=c(2,4), truncQ=2, rm.phix=TRUE,
                                       compress=TRUE, multithread=FALSE )
                       } ),
          errF1 = map(filtF1, ~ learnErrors(.x, multithread=FALSE,verbose = 0)),     # Calculate errors
          errR1 = map(filtR1, ~ learnErrors(.x, multithread=FALSE,verbose = 0)),
          derepF1 = map(filtF1, derepFastq),                   # dereplicate seqs
          derepR1 = map(filtR1, derepFastq),
          dadaF1  = map2(derepF1,errF1, ~ dada(.x, err = .y, pool=TRUE, multithread = TRUE)),  # dada2
          dadaR1  = map2(derepR1,errR1, ~ dada(.x, err = .y, pool=TRUE, multithread = TRUE)),
          mergers = pmap(.l = list(dadaF1,derepF1, dadaR1,derepR1),                 # merge things
                         .f = mergePairs ))

if (keep.mid.files==TRUE){
  write_rds(output.dada2, path = here(outdir, "output.halfway.rds"))}
```

<br>

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. 
```{r}
seqtab <- makeSequenceTable(output.dada2$mergers)
dim(seqtab)
```
87 | 540 -- without pooled processing
87 | 540 -- with pooled processing
87 | 542 -- with pooled processing and allowing a maxEE(2,4) instead of (2,2)
<br>

Inspect distribution of sequence lengths:
```{r echo=FALSE}
table(nchar(getSequences(seqtab)))

table(nchar(getSequences(seqtab))) %>% as.data.frame() %>%
  mutate(Length=as.character(Var1),
         Length=as.numeric(Length)) %>%
  ggplot( aes(x=Length,y=Freq)) +
  geom_col() + theme_bw()
```
```{r}
write.csv(table(nchar(getSequences(seqtab))) %>% as.data.frame() %>%
  mutate(Length=as.character(Var1),
         Length=as.numeric(Length)), file=here(outdir,'dada2_pooled-process-maxEE24_filtered_seqLengths.csv'))
```
<br>


## Remove chimeras

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", verbose=TRUE)
dim(seqtab.nochim)

seqtab.nochim.df <- as.data.frame(seqtab.nochim)
```
idependent / pooled processing:

Identified 114 bimeras out of 540 input sequences.
[1]  87 | 426



maxEE(2,4) + pooled processing:

Identified 119 bimeras out of 542 input sequences.
[1]  87 | 423
<br>

## Write output

Copy the metadata so it is all in one place
```{r}
cutadapt.meta.output %>% write_csv(here(outdir,"dada2.metadata.csv"))
```
<br>

Output file names
```{r}
conv_file <- here(outdir,"hash_key_maxEE.csv")
conv_file.fasta <- here(outdir,"hash_key_maxEE.fasta")
ASV_file <-  here(outdir,"ASV_table_maxEE.csv")
```
<br>

If using hashes, set up the output table with hash IDs and write it out.
```{r}
if (hash==TRUE)
{conv_table <- tibble( Hash = "", Sequence ="")
  map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto")) -> Hashes
  conv_table <- tibble (Hash = Hashes,
                        Sequence = colnames(seqtab.nochim.df))
  seqtab.nochim.hashes.df <- seqtab.nochim.df
  colnames(seqtab.nochim.hashes.df) <- Hashes

  write_csv(conv_table, conv_file) # write the table into a file
  write.fasta(sequences = as.list(conv_table$Sequence),
              names     = as.list(conv_table$Hash),
              file.out = conv_file.fasta)
  seqtab.nochim.hashes.df <- bind_cols(cutadapt.meta.output %>%
                                         select(Sample_name, Locus),
                                       sample.names.df,
                                seqtab.nochim.hashes.df)
  seqtab.nochim.hashes.df %>%
    pivot_longer(cols = c(- Sample_name, -sample_id, - Locus),
                 names_to = "Hash",
                 values_to = "nReads") %>%
    filter(nReads > 0) -> current_asv
  write_csv(current_asv, ASV_file)    }else{
    #What do we do if you don't want hashes: two things - Change the header of the ASV table, write only one file
    seqtab.nochim.df %>%
      pivot_longer(cols = c(- Sample_name, - Locus),
                   names_to = "Sequence",
                   values_to = "nReads") %>%
      filter(nReads > 0) -> current_asv
    write_csv(current_asv, ASV_file)
  }
```
<br>

# QC: Track reads

Get the number of reads at each step. 

```{r include=FALSE}
getN <- function(x) sum(getUniques(x))
```

```{r}
qc.dat <- output.dada2 %>%
  select(-file1, -file2, -filtF1, -filtR1, -errF1, -errR1, -derepF1, -derepR1) %>%
  mutate_at(.vars = c("dadaF1", "dadaR1", "mergers"),
            ~ sapply(.x,getN)) %>%
  #  pull(outFs) -> test
  mutate(input = map_dbl(outFs, ~ .x[1]),
         filtered = map_dbl(outFs, ~ .x[2]),
         tabled  = rowSums(seqtab),
         nonchim = rowSums(seqtab.nochim)) %>%
  select(Sample_name,
         Locus,
         input,
         filtered,
         denoised_F = dadaF1,
         denoised_R = dadaR1,
         merged = mergers,
         tabled,
         nonchim)
write_csv(qc.dat, here(outdir,"dada2_qc_summary.csv"))

## drop
# if (keep.mid.files==FALSE){
#   unlink(here(filt.dir), recursive = T)
# }
```
<br>

Make output_summaryfig
```{r eval=FALSE}
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`, group =  Sample_name, color = Sample_name)) +
  geom_line() +
  guides(color = "none")
```
```{r eval=FALSE}
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  mutate (group = ifelse(Sample_name %in% c(94,95,96), "Control", "Sample")) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`/1000, color = group)) +
  geom_boxplot() +
  guides(color = "none") + theme_bw()
```

