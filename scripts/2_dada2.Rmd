---
title: "DADA2"
subtitle: "last run: `r format(Sys.time(), '%B %d, %Y')`"
author: "M Fisher (from Eily, Moncho)"
date: '2022-06-19'
output: 
  html_document:
    toc: yes
---


# Description

Run **DADA2** [tutorial here](https://benjjneb.github.io/dada2/tutorial.html) in order to get an amplicon sequence variant (ASV) table, which records the number of times each exact amplicon sequence variant was observed in each sample. 

Certain decisions have to be made throughout the script, so *do not just knit this script with the existing values*. Go through each code chunk in R first, then knit. Certain code chunks will not re-run when the script is knitted, to avoid over-writing existing files.

Input: demultiplexed fastq files, without barcodes / adapters / primers. 



<br>

# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("here")) {install.packages("here")}
if(!require("tidyverse")) {install.packages("tidyverse")}
if(!require("magrittr")) {install.packages("magrittr")}
if(!require("digest")) {install.packages("digest")}
if(!require("seqinr")) {install.packages("seqinr")}
# if(!requireNamespace("BiocManager", quietly = TRUE)){install.packages("BiocManager")}
# BiocManager::install("dada2", version = "3.10")
library(dada2)
library(digest)
library(seqinr)
```
<br>

User directories
```{r set up }
# root directory for cutadapt
cutadapt_dir <- "data/cutadapt"
# output directory
outdir <- "data/dada2"
```

User inputs
```{r}
run.num = 2

hash = TRUE  # rarely do you want hash = false (EA)

keep.mid.files = FALSE # I find I never look at these / use these and they just take up space (EA)
```
<br>
<br>


# Prep for DADA2

```{r message=FALSE, warning=FALSE}
run_cutadapt_dir = paste0(cutadapt_dir, "/noprimers")
```
<br>

read in sequencing metadata. set default trim length based on primer. 
```{r message=FALSE}
cutadapt.meta <- read_csv(here(run_cutadapt_dir, paste0("output.metadata.csv")))
marker        <- unique(cutadapt.meta$Locus)
print(marker)
```
```{r echo=FALSE}
if(marker=="Leray" | marker=="LerayXT"){
  trimming.length.r1 = 250
  trimming.length.r2 = 200
  message("trim lengths set as (r1,r1): ", trimming.length.r1, ",",trimming.length.r2)
} else if(marker=="BF3"){
  trimming.length.r1 = 260
  trimming.length.r2 = 200
} else{
  message("please manually enter trim length for this marker.")
}
```
<br>

read in file names.
```{r}
fnFs <- sort(list.files(path=here(run_cutadapt_dir), pattern="_R1_001.fastq.fastq", full.names = TRUE))
fnRs <- sort(list.files(path=here(run_cutadapt_dir), pattern="_R2_001.fastq.fastq", full.names = TRUE))
```
<br>

get the sample names, which will be used to name the filtered files.
```{r}
# sample.names <- marker.meta$Sample_name
prefix <- paste0("Locus_Leray_")
suffix <- paste0("_L001_R1_001.fastq.fastq")
sample.names.df <- (cutadapt.meta %>% dplyr::select(file1) %>%
  mutate(sample_id=str_remove(file1,prefix)) %>%
  mutate(sample_id=str_remove(sample_id,suffix)) %>%
  separate(col=sample_id, into=c("sample_id","sample.num"), sep="_S")) %>% dplyr::select(sample_id)
sample.names <- as.character(sample.names.df$sample_id)
```
<br>

write output directory path for filtered files in the run's cutadapt folder.
```{r}
filt.dir <- paste0(cutadapt_dir, "/noprimers_filtered")
```
<br>

create directories if they don't exist
```{r}
if(!dir.exists(here(filt.dir))){
  dir.create(path = here(filt.dir),recursive = T)
}
if(!dir.exists(here(outdir))){
  dir.create(path = here(outdir),recursive = T)
}
```
<br>

Double check the quality scores
Plot forward read quality scores, one sample per group
```{r echo=FALSE}
plotQualityProfile(fnFs[c(1,2,6,19,25)])
```
<br>

Plot forward read quality scores, one sample per group
```{r echo=FALSE}
plotQualityProfile(fnRs[c(1,2,6,19,25)])
```
<br>


manually enter trim lengths - if there isn't a default for the marker, or if the defaults are too long based on the quality scores (see script 0_qc)
```{r eval=FALSE}
trimming.length.r1 = 250
trimming.length.r2 = 190
```
<br>
<br>

# DADA2

## Filter and trim

- `truncLen` truncates the sequence length, and should be based on per-base quality scores. I'm using the length that Eily suggested , 120bp.
- `maxEE` discards reads with high "expected errors" = $\sum(10^{-Q/10})$
- `rm.phix` removes reads from PhiX spike
- use `compress` to gzip the output fastqs

```{r}
output.dada2 <- cutadapt.meta %>%
  #filter(rc == 1) %>% # ONLY SELECT THE BACKWARDS ONES (1) OR FORWARDS ONES (0)
  mutate(basename=sample.names) %>%
  mutate(file1  = here(run_cutadapt_dir, file1),
         file2  = here(run_cutadapt_dir, file2),
         filtF1 = here(filt.dir, paste0(basename, "_F1_filt.fastq.gz")),
         filtR1 = here(filt.dir, paste0(basename, "_R1_filt.fastq.gz"))) %>%
  select(-basename) %>% 
  mutate (outFs = pmap(.l= list (file1, filtF1, file2, filtR1),
                       .f = function(a, b, c, d) {
                         filterAndTrim(a,b,c,d,
                                       truncLen=c(trimming.length.r1,trimming.length.r2),
                                       maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                                       compress=TRUE, multithread=FALSE )
                       } ),
          errF1 = map(filtF1, ~ learnErrors(.x, multithread=FALSE,verbose = 0)),     # Calculate errors
          errR1 = map(filtR1, ~ learnErrors(.x, multithread=FALSE,verbose = 0)),
          derepF1 = map(filtF1, derepFastq),                   # dereplicate seqs
          derepR1 = map(filtR1, derepFastq),
          dadaF1  = map2(derepF1,errF1, ~ dada(.x, err = .y, multithread = FALSE)),  # dada2
          dadaR1  = map2(derepR1,errR1, ~ dada(.x, err = .y, multithread = FALSE)),
          mergers = pmap(.l = list(dadaF1,derepF1, dadaR1,derepR1),                 # merge things
                         .f = mergePairs ))

if (keep.mid.files==TRUE){
  write_rds(output.dada2, path = here(output.dir, "output.halfway.rds"))}
```
<br> 

<br>

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. 
```{r}
seqtab <- makeSequenceTable(output.dada2$mergers)
dim(seqtab)
```
86 | 4973
<br>

Inspect distribution of sequence lengths:
```{r echo=FALSE}
table(nchar(getSequences(seqtab)))

table(nchar(getSequences(seqtab))) %>% as.data.frame() %>%
  mutate(Length=as.character(Var1),
         Length=as.numeric(Length)) %>%
  ggplot( aes(x=Length,y=Freq)) +
  geom_col() + theme_bw()
```
[saved to xlsx file]
<br>


## Remove chimeras

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", verbose=TRUE)
dim(seqtab.nochim)

seqtab.nochim.df <- as.data.frame(seqtab.nochim)
```
Identified 2206 bimeras out of 5181 input sequences.
86 | 2931
<br>

## Write output

Copy the metadata so it is all in one place
```{r}
cutadapt.meta %>% write_csv(here(output.dir,"dada2.metadata.csv"))
```
<br>

Output file names
```{r}
conv_file <- here(output.dir,"hash_key.csv")
conv_file.fasta <- here(output.dir,"hash_key.fasta")
ASV_file <-  here(output.dir,"ASV_table.csv")
```
<br>

If using hashes, set up the output table with hash IDs and write it out.
```{r}
if (hash==TRUE)
{conv_table <- tibble( Hash = "", Sequence ="")
  map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto")) -> Hashes
  conv_table <- tibble (Hash = Hashes,
                        Sequence = colnames(seqtab.nochim.df))
  seqtab.nochim.hashes.df <- seqtab.nochim.df
  colnames(seqtab.nochim.hashes.df) <- Hashes

  write_csv(conv_table, conv_file) # write the table into a file
  write.fasta(sequences = as.list(conv_table$Sequence),
              names     = as.list(conv_table$Hash),
              file.out = conv_file.fasta)
  seqtab.nochim.hashes.df <- bind_cols(cutadapt.meta %>%
                                         select(Sample_name, Locus),
                                       sample.names.df,
                                seqtab.nochim.hashes.df)
  seqtab.nochim.hashes.df %>%
    pivot_longer(cols = c(- Sample_name, -sample_id, - Locus),
                 names_to = "Hash",
                 values_to = "nReads") %>%
    filter(nReads > 0) -> current_asv
  write_csv(current_asv, ASV_file)    }else{
    #What do we do if you don't want hashes: two things - Change the header of the ASV table, write only one file
    seqtab.nochim.df %>%
      pivot_longer(cols = c(- Sample_name, - Locus),
                   names_to = "Sequence",
                   values_to = "nReads") %>%
      filter(nReads > 0) -> current_asv
    write_csv(current_asv, ASV_file)
  }
```
<br>

# QC: Track reads

Get the number of reads at each step. 

```{r include=FALSE}
getN <- function(x) sum(getUniques(x))
```

```{r}
qc.dat <- output.dada2 %>%
  select(-file1, -file2, -filtF1, -filtR1, -errF1, -errR1, -derepF1, -derepR1) %>%
  mutate_at(.vars = c("dadaF1", "dadaR1", "mergers"),
            ~ sapply(.x,getN)) %>%
  #  pull(outFs) -> test
  mutate(input = map_dbl(outFs, ~ .x[1]),
         filtered = map_dbl(outFs, ~ .x[2]),
         tabled  = rowSums(seqtab),
         nonchim = rowSums(seqtab.nochim)) %>%
  select(Sample_name,
         Locus,
         input,
         filtered,
         denoised_F = dadaF1,
         denoised_R = dadaR1,
         merged = mergers,
         tabled,
         nonchim)
write_csv(qc.dat, here(output.dir,"dada2_qc_summary.csv"))

## drop
if (keep.mid.files==FALSE){
  unlink(here(filt.dir), recursive = T)
}
```
<br>

Make output_summaryfig
```{r eval=FALSE}
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`, group =  Sample_name, color = Sample_name)) +
  geom_line() +
  facet_wrap(~Sample_name) +
  guides(color = "none")
```
```{r eval=FALSE}
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  mutate (group = ifelse(Sample_name %in% c(94,95,96), "Control", "Sample")) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`/1000, color = group)) +
  geom_boxplot() +
  guides(color = "none") + theme_bw()
```

